import torchvision
import torchcelerate
 
model = torchvision.models.resnet18()
model.eval() # set the model to inference mode
 
# Optimize the model for fast, cross-platform inference. To enable the best optimization we need to give it some information.
x = torch.randn(1, 1, 224, 224, requires_grad=True)
optimized_model = torchcelerate.optimize(model,
                                         x,
                                         input_names = ['input'],
                                         output_names = ['output'],
                                         dynamic_axes={'input' : {0 : 'batch_size'},
                                                       'output' : {0 : 'batch_size'}})
 
# You can run the optimized model to verify accuracy and performance
outputs = optimized_model(inputs);
 
# Now you can save it out for use in C++, C#, WinML, Java, edge devices, etc.
optimized_model.serialize("model.onnx")
